1. Create ‘sales’ Data set having 5 columns namely: ID, TV, Radio, Newspaper and Sales.(random
500 entries) Build a linear regression model by identifying independent and target variable. Split the
variables into training and testing sets. then divide the training and testing sets into a 7:3 ratio,
respectively and print them. Build a simple linear regression model.



import warnings
warnings.filterwarnings('ignore')

# Import the numpy and pandas package
import numpy as np
import pandas as pd

# Read the given CSV file, and view some sample records
advertising = pd.read_csv("Company_data.csv")
advertising

# Shape of our dataset
advertising.shape

# Info our dataset
advertising.info()

# Describe our dataset
advertising.describe()
# Import matplotlib and seaborn libraries to visualize the data
import matplotlib.pyplot as plt 
import seaborn as sns

# Using pairplot we'll visualize the data for correlation
sns.pairplot(advertising, x_vars=['TV', 'Radio','Newspaper'], 
             y_vars='Sales', size=4, aspect=1, kind='scatter')
plt.show()
#As we can see from the above graphs, the TV column seems most correlated to Sales.
#Let’s perform the simple linear regression model using TV as our feature variable.
#Performing Simple Linear Regression
Equation of simple linear regression
y = c + mX
#In our case:
y = c + m * TV
#We’ll perform simple linear regression in four steps.
#Create X and y
#Create Train and Test set
#Train your model
#Evaluate the model
#The independent variable represents X, and y represents the target variable in a simple linear regression model.
# Creating X and y
X = advertising['TV']
y = advertising['Sales']
#Create Train and Test sets
We need to split our variables into training and testing sets. Using the training set, we’ll build the model and perform the model on the testing set. We’ll divide the training and testing sets into a 7:3 ratio, respectively.

#We’ll split the data by importing train_test_split from the sklearn.model_selection library.
# Splitting the varaibles as training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)
# Take a look at the train dataset
X_train
y_train
#Building and training the model
#Using the following two packages, we can build a simple linear regression model.
statsmodel
sklearn
# Importing Statsmodels.api library from Stamodel package
import statsmodels.api as sm
# Adding a constant to get an intercept
X_train_sm = sm.add_constant(X_train)
#Once we’ve added constant, we can fit the regression line using OLS (Ordinary Least Square) method present in the statsmodel. After that, we’ll see the parameters, i.e., c and m of the straight line.
# Fitting the resgression line using 'OLS'
lr = sm.OLS(y_train, X_train_sm).fit()
# Printing the parameters
lr.params
# Performing a summary to list out all the different parameters of the regression line fitted
lr.summary()
#From the parameters, we got the values of the intercept and the slope for the straight line. The equation of the line is,
Sales = 6.948 + 0.054 * TV
# Visualizing the regression line
plt.scatter(X_train, y_train)
plt.plot(X_train, 6.948 + 0.054*X_train, 'r')
plt.show()
#Residual Analysis
#One of the major assumptions of the linear regression model is the error terms are normally distributed.
Error = Actual y value - y predicted value
#We have to predict the y value from the training dataset of X using the predict attribute.
# Predicting y_value using traingn data of X
y_train_pred = lr.predict(X_train_sm)

# Creating residuals from the y_train data and predicted y_data
res = (y_train - y_train_pred)
Now, let’s plot the histogram of the residuals and see whether it looks like normal distribution or not.
# Plotting the histogram using the residual values
fig = plt.figure()
sns.distplot(res, bins = 15)
plt.title('Error Terms', fontsize = 15)
plt.xlabel('y_train - y_train_pred', fontsize = 15)
plt.show()
#Predictions on the Test data or Evaluating the model
# Adding a constant to X_test
X_test_sm = sm.add_constant(X_test)

# Predicting the y values corresponding to X_test_sm
y_test_pred = lr.predict(X_test_sm)

# Printing the first 15 predicted values
y_test_pred
Now, let’s calculate the R² value for the above-predicted y-values. We can do that by merely importing the r2_score library from sklearn.metrics package.
# Importing r2_square
from sklearn.metrics import r2_score

# Checking the R-squared value
r_squared = r2_score(y_test, y_test_pred)
r_squared
Let’s visualize the line on the test data.
# Visualize the line on the test set
plt.scatter(X_test, y_test)
plt.plot(X_test, y_test_pred, 'r')
plt.show()

# Splitting the data into train and test
from sklearn.model_selection import train_test_split
X_train_lm, X_test_lm, y_train_lm, y_test_lm = train_test_split(X, y, train_size = 0.7, 
                                                                test_size = 0.3, random_state = 100)
#For simple linear regression, we need to add a column to perform the regression fit properly.
# Shape of the train set without adding column
X_train_lm.shape

# Adding additional column to the train and test data
X_train_lm = X_train_lm.values.reshape(-1,1)
X_test_lm = X_test_lm.values.reshape(-1,1)

print(X_train_lm.shape)
print(X_test_lm.shape)
#Now, let’s fit the line to the plot importing the LinearRegression library from the sklearn.linear_model.
from sklearn.linear_model import LinearRegression

# Creating an object of Linear Regression
lm = LinearRegression()

# Fit the model using .fit() method
lm.fit(X_train_lm, y_train_lm)
Now, let’s find the coefficients of the model.
# Intercept value
print("Intercept :",lm.intercept_)

# Slope value
print('Slope :',lm.coef_)
#After that, we’ll make the predictions and on the data and evaluate the model by comparing the R² values.
# Making Predictions of y_value
y_train_pred = lm.predict(X_train_lm)
y_test_pred = lm.predict(X_test_lm)

# Comparing the r2 value of both train and test data
print(r2_score(y_train,y_train_pred))
print(r2_score(y_test,y_test_pred))




2. Create ‘realestate’ Data set having 4 columns namely: ID,flat, houses and purchases (random 500
entries). Build a linear regression model by identifying independent and target variable. Split the
variables into training and testing sets and print them. Build a simple linear regression model for
predicting purchases.


import pandas as pd
import seaborn as sns
import sklearn

a=pd.read_csv(&quot;../input/real-estate/realestate.csv&quot;)
a
a.columns
a1=a.iloc[:,2:7]
a1
y=a.iloc[:,7:8]
y
a.info()
a.describe()
sns.pairplot(a)
#normalizing the data:
from sklearn import preprocessing
a_nor=preprocessing.normalize(a)
a_nor=pd.DataFrame(a_nor)
a_nor
a_nor.columns=a.columns
a_nor
#splitting x and y:
x=a_nor.iloc[:,2:7]
x
y=a_nor.iloc[:,7:8]
y
#paiplot for a_nor:
sns.pairplot(a_nor)
sns.pairplot(x)
x.corr()
a_nor.corr()

#splitting training and test data set:
from sklearn import model_selection
from sklearn import linear_model
x_train,x_test,y_train,y_test=model_selection.train_test_split(x,y,test_size=0.2)
x_train.shape
x_test.shape
y_train.shape
y_test.shape
x_train
#applying linear regression:
lm=linear_model.LinearRegression()
model=lm.fit(x_train,y_train)
model.coef_
model.intercept_
pred=lm.predict(x_train)
pred
#checking accuracy using r2:
from sklearn.metrics import r2_score
r2_score(pred,y_train)
predd=lm.predict(x_test)
r2_score(predd,y_test)




3. Create ‘User’ Data set having 5 columns namely: User ID, Gender, Age, EstimatedSalary and
Purchased. Build a logistic regression model that can predict whether on the given parameter a
person will buy a car or not.


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv(r'F:\T.Y.B.SC(CS)-SEM 6\Prac DA\Social_Network_Ads.csv')

#In our Data set we’ll consider Age and  EstimatedSalary as Independent variable
#and Purchased as Dependent Variable.

X = dataset.iloc[:, [2,3]].values
y = dataset.iloc[:, 4].values

#we’ll split our Data set into Training Data and Test Data.
#Training data will be used to train our
#Logistic model and Test data will be used to validate our model.

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

#we’ll do feature scaling to scale our data between 0 and 1 to get better accuracy.
#Here Scaling is important because there is a huge difference between Age and EstimatedSalay.

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

#Fitting Logistic Regression to the Training Set

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)

#Predicting the Test set results

y_pred = classifier.predict(X_test)

#Using Confusion matrix we can get accuracy of our model.

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

print(cm)

#Use cm to calculate accuracy as shown below:
#(65+24)/100=0.89=89%

#Visualizing the Training Set result

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1,   step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#Visualizing the Test Set result

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()